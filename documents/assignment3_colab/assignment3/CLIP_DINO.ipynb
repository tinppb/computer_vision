{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S7MV4XqiiQ0g"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = \"cs231n/assignments/assignment3/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0CPS6Gm27O8"},"outputs":[],"source":["# This downloads the COCO dataset to your Drive if it doesn't already exist\n","# (you should already have this dataset from a previous notebook!)\n","# Uncomment the following if you don't have it.\n","# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","# !bash get_coco_captioning.sh\n","# %cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mkg1F0J081A"},"outputs":[],"source":["# Some useful python libraries\n","! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git\n","! pip install decord"]},{"cell_type":"markdown","metadata":{"id":"k-IumfyxjUZT"},"source":["# State-of-the-Art Pretrained Image Models\n","\n","In the previous exercise, you learned about [SimCLR](https://arxiv.org/abs/2002.05709) and how contrastive self-supervised learning can be used to learn meaningful image representations. In this notebook, we will explore two more recent models that also aim to learn high-quality visual representations and have demonstrated strong and robust performance on a variety of downstream tasks.\n","\n","\n","First, we will examine the [CLIP](https://github.com/openai/CLIP) model. Like SimCLR, CLIP uses a contrastive learning objective, but instead of contrasting two augmented views of the same image, it contrasts two different modalities: text and image. To train CLIP, OpenAI collected a large dataset of ~400M image-text pairs from the internet, including sources like Wikipedia and image alt text. The resulting model learns rich, high-level image features and has achieved impressive zero-shot performance on many vision benchmarks.\n","\n","Next, we will explore [DINO](https://github.com/facebookresearch/dino), a self-supervised learning method for vision tasks that applies contrastive learning in a self-distillation framework with multi-crop augmentation strategy. The authors showed that the features learned by DINO ViTs are fine-grained and semantically rich with explicit information about the semantic segmentation of the image.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JZJOAUKaoze_"},"source":["# CLIP\n","\n","As explained above, CLIP's training objective incorporates both text and images, building upon the principles of contrastive learning. Consider this quote from the SimCLR notebook:\n",">The goal of the contrastive loss is to maximize agreement between the final vectors **$z_i = g(h_i)$** and **$z_j = g(h_j)$**.\n","\n","Similarly, CLIP is trained to maximize agreement between two vectors. However, because these vectors come from different modalities, CLIP uses two separate encoders: a transformer-based Text Encoder and a Vision Transformer (ViT)-based Image Encoder. Note that some smaller, more efficient versions of CLIP use a ResNet as the Image Encoder instead of a ViT.\n","\n","Run the cell below to visualize the training and inference pipeline of CLIP.\n","\n","During the pretraining phase, each batch consists of multiple images along with their corresponding captions. Each image is independently processed by an Image Encoder—typically a visual model like a Vision Transformer (ViT) or a Convolutional Neural Network (ConvNet)—which produces an image embedding $I_n$. Likewise, each caption is independently processed by a Text Encoder to generate a corresponding text embedding $T_n$. Next, we compute the pairwise similarities between all image-text combinations, meaning each image is compared with every caption, and vice versa. The training objective is to maximize the similarity scores along the diagonal of the resulting similarity matrix -- that is, the scores for the matching image-caption pairs $(I_n, T_n)$.  Through backpropagation, the model learns to assign higher similarity scores to true matches than to mismatched pairs.\n","\n","Through this setup, CLIP effectively learns to represent images and texts in a shared latent space. In this space, semantic concepts are encoded in a modality-independent way, enabling meaningful cross-modal comparisons between visual and textual inputs.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyNGHPpf4kIL"},"outputs":[],"source":["from IPython.display import Image as ColabImage\n","ColabImage(f'/content/drive/My Drive/{FOLDERNAME}/CLIP.png')"]},{"cell_type":"markdown","metadata":{"id":"96UkICCbmoX-"},"source":["**Inline Question 1** -\n","\n","Why does CLIP's learning depend on the batch size? If the batch size is fixed, what strategy can we use to learn rich image features?\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lkVe0nRuxXIP"},"source":["# Loading COCO dataset\n","\n","We'll use the same captioning dataset you used to train your RNN captioning model, but instead of generating the captions lets see if we can match each image to the correct caption."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUkEEQ2YyTSo"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import time, os, json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import clip\n","import torch\n","from tqdm.auto import tqdm\n","\n","from PIL import Image\n","from cs231n.clip_dino import *\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error.\"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-10, np.abs(x) + np.abs(y))))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jq0KZifyNDA"},"outputs":[],"source":["from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n","from cs231n.image_utils import image_from_url"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AT-Ux6N8ovqZ"},"outputs":[],"source":["# Load COCO data from disk into a dictionary.\n","# this is the same dataset you used for the RNN captioning notebook :)\n","data = load_coco_data(pca_features=True)\n","\n","# Print out all the keys and values from the data dictionary.\n","for k, v in data.items():\n","    if type(v) == np.ndarray:\n","        print(k, type(v), v.shape, v.dtype)\n","    else:\n","        print(k, type(v), len(v))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZYimNYozKxh"},"outputs":[],"source":["# we're just using the loaded captions from COCO, so we need to decode them and get rid of the special tokens.\n","decoded_captions= []\n","for caption in data['val_captions']:\n","  caption = decode_captions(caption, data['idx_to_word'])\\\n","    .replace('<START>', '')\\\n","    .replace('<END>', '')\\\n","    .replace('<UNK>', '')\\\n","    .strip()\n","  decoded_captions.append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bdpT8K52ukP"},"outputs":[],"source":["# lets get 10 examples\n","mask = np.array([135428, 122586, 122814, 133173, 176639, 163828,  98169,   6931,\n","        19488, 175760])\n","first_captions = [decoded_captions[elem] for elem in mask]\n","\n","img_idxs = data['val_image_idxs'][mask]       # the images the captions refer to\n","first_images   = [image_from_url(data['val_urls'][j]) for j in img_idxs]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-Zvmbg_N42T"},"outputs":[],"source":["for i, (caption, image) in enumerate(zip(first_captions, first_images)):\n","    plt.imshow(image)\n","    plt.axis('off')\n","    caption_str = caption\n","    plt.title(caption_str)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"I144vQb4sXVW"},"source":["# Running the CLIP Model\n","\n","First we'll use the pretrained CLIP model to extract features from the texts and images separetely."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNWKDxLs29Dd"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-inoZiP79TJ"},"outputs":[],"source":["# You can check the model layers by printing the model.\n","# CLIP's model code is available at https://github.com/openai/CLIP/tree/main/clip\n","# print(clip_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znkbk9r1zjkC"},"outputs":[],"source":["# First, we encode the captions into vectors in the shared embedding space.\n","# Since we're using a Transformer as the text encoder, we need to tokenize the text first.\n","text_tokens = clip.tokenize(first_captions).to(device)\n","with torch.no_grad():\n","    text_features = clip_model.encode_text(text_tokens)\n","\n","# Sanity check, print the shape\n","print(text_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vvRlMrR2XZk"},"outputs":[],"source":["# Then, we encode the images into the same embedding space.\n","processed_images = [\n","    clip_preprocess(Image.fromarray(img)).unsqueeze(0)\n","    for img in first_images\n","]\n","images_tensor = torch.cat(processed_images, dim=0).to(device)\n","\n","with torch.no_grad():\n","    image_features = clip_model.encode_image(images_tensor)\n","\n","# sanity check, print the shape\n","print(image_features.shape)"]},{"cell_type":"markdown","metadata":{"id":"G3IfwGD__Xb8"},"source":["Open `cs231n/clip_dino.py` and implement `get_similarity_no_loop` to compute similarity scores between text features and image features. Test your implementation below, you should see relative errors less than 1e-5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWRW5AG4_X94"},"outputs":[],"source":["from cs231n.clip_dino import get_similarity_no_loop\n","torch.manual_seed(231)\n","np.random.seed(231)\n","M, N, D = 5, 6, 10\n","\n","test_text_features = torch.randn(N, D)\n","test_image_features = torch.randn(M, D)\n","out = get_similarity_no_loop(test_text_features, test_image_features)\n","\n","expected_out = np.array([\n","    [ 0.1867811 , -0.23494351,  0.44155994, -0.18950461,  0.00100103],\n","    [ 0.17905031, -0.25469488, -0.64330417,  0.25097957, -0.09327742],\n","    [-0.4407011 , -0.4365381 ,  0.32857686, -0.3765278 ,  0.01049389],\n","    [ 0.24815483,  0.42157224, -0.08459304,  0.14132318, -0.26935193],\n","    [ 0.02309848, -0.01441101,  0.5469337 ,  0.6018773 ,  0.21581158],\n","    [ 0.41579214, -0.014449  , -0.7242257 ,  0.39348006,  0.0822239 ],\n","]).astype(np.float32)\n","\n","print(\"relative error: \", rel_error(out.numpy(), expected_out))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8t54B7xh4PL-"},"outputs":[],"source":["# Let's visualize the similarities between our batch of images and their captions.\n","\n","similarities = get_similarity_no_loop(text_features, image_features).cpu().detach().numpy()\n","\n","plt.figure(figsize=(20, 14))\n","plt.imshow(similarities, vmin=0.1, vmax=0.3)\n","plt.yticks(range(len(text_features)), first_captions, fontsize=18)\n","plt.xticks([])\n","for i, image in enumerate(first_images):\n","    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n","for x in range(similarities.shape[1]):\n","    for y in range(similarities.shape[0]):\n","        plt.text(x, y, f\"{similarities[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n","\n","for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n","    plt.gca().spines[side].set_visible(False)\n","\n","plt.xlim([-0.5, len(image_features) - 0.5])\n","plt.ylim([len(text_features) + 0.5, -2])\n","\n","plt.title(\"Cosine similarity between text and image features\", size=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JnNuJaABp8Nl"},"source":["# Zero Shot Classifier\n","\n","You will be able to see a high similarity between matching image-caption pairs above. We can leverage this property to design an image classifier that doesn't require any labeled data (i.e., a zero-shot classifier). Each class can be represented using an appropriate natural language description, and any input image will be classified into the class whose description has the highest similarity with the image in CLIP's embedding space."]},{"cell_type":"markdown","metadata":{"id":"2lDbkKaQY8Gd"},"source":["Implement `clip_zero_shot_classifier` in `cs231n/clip_dino.py` and test it below. You should be able to see the following predictions:\n","\n","['a person', 'an animal', 'an animal', 'food', 'a person', 'a landscape', 'other', 'other', 'other', 'a person']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM_WEqsYrCx9"},"outputs":[],"source":["from cs231n.clip_dino import clip_zero_shot_classifier\n","\n","classes = [\"a person\", \"an animal\", \"food\", \"a landscape\", \"other\"]\n","\n","pred_classes = clip_zero_shot_classifier(\n","    clip_model, clip_preprocess, first_images, classes, device)\n","\n","print(pred_classes)"]},{"cell_type":"markdown","metadata":{"id":"yW5xwHdWZlzl"},"source":["Run the cell below to visualize the predictions. As you can see, CLIP offers a straightforward way to perform reasonable zero-shot classification across any class taxonomy.\n","\n","CLIP was the first model to outperform standard supervised training on ImageNet classification without using any ImageNet images or labels (The original CLIP paper has many such interesting experiments and analysis).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"om4mnnVXZKAL"},"outputs":[],"source":["# Visualize the zero shot predictions\n","for i, (pred_class, image) in enumerate(zip(pred_classes, first_images)):\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title(pred_class)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zVCDz3tyfaAD"},"source":["# Image Retrieval using CLIP\n","\n","Just as we used CLIP to retrieve the matching class name for each image, we can also use it to retrieve matching images from text inputs (semantic image retrieval). Implement the `CLIPImageRetriever` in `cs231n/clip_dino.py` and test it by running the two cells below. The expected top 2 outputs for each query are provided in the comments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"077RtVSAfaXd"},"outputs":[],"source":["from cs231n.clip_dino import CLIPImageRetriever\n","clip_retriever = CLIPImageRetriever(clip_model, clip_preprocess, first_images, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"609IYhTzkMBF"},"outputs":[],"source":["query = \"sports\"  # tennis, skateboard\n","# query = \"black and white\"  # bathroom, zerbas\n","img_indices = clip_retriever.retrieve(query)\n","\n","for img_index in img_indices:\n","    plt.imshow(first_images[img_index])\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z1gVu2VDogLb"},"source":["**Inline Question 2** -\n","\n","CLIP learns to align image and text representations in a shared latent space using a contrastive loss. How would you extend this idea to more than two modalities?\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"otX-grqhmRAN"},"source":["# DINO\n","\n","As mentioned earlier, models trained with vanilla contrastive learning methods such as SimCLR and CLIP require very large batch sizes. This makes them computationally expensive and limits their accessibility. Subsequent works, like [BYOL](https://arxiv.org/abs/2006.07733), propose an alternative approach that avoids the need for numerous negative samples by using a student-teacher framework. This method performs surprisingly well and was later adopted by [DINO](https://arxiv.org/abs/2104.14294) .\n","\n","Similar to SimCLR, DINO is trained to maximize the agreement between two vectors derived from different views of the same image. However, unlike SimCLR, DINO uses two separate encoders which are trained differently. The student network is updated via backpropagation to match the outputs of the teacher network. The teacher network is not updated via backpropagation; instead, its weights are updated using an exponential moving average (EMA) of the student's weights. This means that the teacher model evolves more slowly and provides a stable target for the student to learn from.\n","\n","Run the cell below to visualize the DINO training pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35APMU0nq0Ja"},"outputs":[],"source":["from IPython.display import Image as ColabImage\n","ColabImage(f'/content/drive/My Drive/{FOLDERNAME}/dino.gif')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuXuvIq8th9M"},"outputs":[],"source":["# first let's get rid of the CLIP model that's currently using memory\n","del clip_model\n","# Uncomment the following if you are using GPU runtime\n","# torch.cuda.empty_cache()\n","# torch.cuda.ipc_collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBM480L42ykK"},"outputs":[],"source":["# Load smallest dino model. ViT-S/8. Here ViT-S has ~22M parameters and\n","# works on 8x8 patches.\n","dino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n","dino_model.eval().to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-35sU9tQ35KA"},"outputs":[],"source":["# the image we will be playing around with\n","sample_image = Image.fromarray(first_images[0]).convert(\"RGB\")\n","sample_image"]},{"cell_type":"markdown","metadata":{"id":"jQ-tH24e6P0D"},"source":["# DINO Attention Maps\n","\n","Since the loaded DINO checkpoint is based on the ViT architecture, we can visualize what each attention head is focusing on. The code below generates heatmaps showing which patches of the original image the [CLS] token attends to across the various heads in the final layer. Although this model was trained using a self-supervised objective without any explicit instruction to recognize \"structure\" in images, still...\n","\n","Do you notice any patterns?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux-p68vw4fdt"},"outputs":[],"source":["# Preprocess\n","from torchvision import transforms as T\n","transform = T.Compose([\n","    T.Resize((480, 480)),\n","    T.ToTensor(),\n","    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","])\n","img_tensor = transform(sample_image)\n","w, h = img_tensor.shape[1:]\n","img_tensor = img_tensor[None].to(device)\n","\n","# Extract attention\n","with torch.no_grad():\n","    attn = dino_model.get_last_selfattention(img_tensor)[0, :, 0, 1:]\n","nh, tokens = attn.shape\n","w_feat, h_feat = w // 8, h // 8\n","attn = attn.reshape(nh, w_feat, h_feat)\n","attn = torch.nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n","\n","# Plot attention heads\n","fig, axes = plt.subplots(1, nh, figsize=(3 * nh, 3))\n","for i in range(nh):\n","    ax = axes[i] if nh > 1 else axes\n","    ax.imshow(attn[i], cmap='inferno')\n","    ax.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMPaTwfB9e3Y"},"outputs":[],"source":["# Extract patch token features and discard [CLS] token.\n","with torch.no_grad():\n","    all_tokens = dino_model.get_intermediate_layers(img_tensor, n=1)[0]  # (1, 1+N, D)\n","    patch_tokens = all_tokens[:, 1:, :]  # (N, D)\n","\n","print(img_tensor.shape)\n","print(all_tokens.shape)\n","print(patch_tokens.shape)"]},{"cell_type":"markdown","metadata":{"id":"6zQgCDpQ9r3p"},"source":["\n","**Inline Question 3**\n","\n","How do we get the tensor shapes printed above? Explain your answer.\n","\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xG19KjdGCGp-"},"source":["# DINO Features\n","\n","To understand what the model is encoding in each patch, we can visualize the contents of each patch token. Since these embeddings are high-dimensional and difficult to interpret directly, we'll use PCA to identify the directions of highest variance in the feature space.\n","\n","In the next cell, we visualize the three principal directions of variance in the feature space. This reveals the dominant structure that the patch embeddings are capturing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YKYFpoa_XpS"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","np.random.seed(231)\n","\n","# PCA\n","pca = PCA(n_components=3)\n","patch_pca = pca.fit_transform(patch_tokens.cpu().numpy()[0])\n","\n","# Normalize PCA components to [0, 1] for RGB display\n","patch_rgb = (patch_pca - patch_pca.min(0)) / (patch_pca.max(0) - patch_pca.min(0))\n","\n","# Reshape to image grid (60x60, 3)\n","patch_rgb_img = patch_rgb.reshape(60, 60, 3)\n","\n","# Show as image\n","plt.figure(figsize=(6, 6))\n","plt.imshow(patch_rgb_img)\n","plt.axis('off')\n","plt.title(\"Patch Embeddings (PCA → RGB)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tgQR6poMAyOK"},"source":["**Inline Question 4** -\n","\n","What kind of structure do you see in the visualization above? What does it imply when a region consistently appears in a specific color? What does it mean when two regions have distinctly different color? Remember that PCA reveals the directions of highest variance in the feature space across all patches. A patch's color reflects its distinct feature content.\n","\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5J7GZfBPFyFQ"},"source":["# A Simple Segmentation Model over DINO Features\n","\n","In the previous section, we saw that DINO features can provide surprisingly good segmentation cues. Now, let's put that idea to the test by training a simple segmentation model on the [DAVIS dataset](https://davischallenge.org). The DAVIS dataset (Densely Annotated VIdeo Segmentation) was created for video object segmentation tasks. It provides frame-by-frame, pixel-level annotations of objects within videos. For this experiment, we'll train our model using the annotations from just a single frame of a video and see how well it performs on the remaining frames of the same.\n","\n","Our model will be intentionally minimal: we'll extract DINO features per patch and train a lightweight per-patch classifier using only the patches from that one annotated frame. Typically, you would train on the full dataset and evaluate on a separate validation set containing different videos. But here, we will test the one-shot capabilities of DINO features.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6aAbgondcy2"},"outputs":[],"source":["from cs231n.clip_dino import DavisDataset\n","\n","# A helper class to work with DAVIS dataset.\n","# It may take ~5 minutes on the first run of this cell to download the dataset.\n","davis_ds = DavisDataset()\n","\n","# Get a specific test video. Do NOT change this for submission.\n","frames, masks = davis_ds.get_sample(7)\n","num_classes = masks.max() + 1\n","\n","print(frames.shape, masks.shape, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yl0spxJRiAjf"},"outputs":[],"source":["# Get DINO patch features and corresponding class labels for a middle frame\n","train_fi = 40\n","X_train = davis_ds.process_frames(frames[train_fi:train_fi+1], dino_model, device)[0]\n","Y_train = davis_ds.process_masks(masks[train_fi:train_fi+1], device)[0]\n","print(X_train.shape, Y_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"w6S3DAKl_9NE"},"source":["Complete the implementation of the `DINOSegmentation` class in `cs231n/clip_dino.py`, and test it by running the two cells below. You should achieve a mean IoU greater than 0.45 on the first test frame and greater than 0.50 on the last test frame. To prevent overfitting on the training patch features, consider designing a very lightweight model (e.g., a linear layer or a 2-layer MLP) and applying appropriate weight decay.\n","\n","You may use GPU runtime to speed up training and evaluation. Make sure to rerun the entire notebook if you change runtime type."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B31JBmQm-T6"},"outputs":[],"source":["from cs231n.clip_dino import DINOSegmentation, compute_iou\n","torch.manual_seed(231)\n","np.random.seed(231)\n","dino_segmentation = DINOSegmentation(device, num_classes)\n","dino_segmentation.train(X_train, Y_train, num_iters=500)\n","\n","\n","# Test on first, middle, and last frame\n","ious = []\n","test_fis = [0, train_fi, 98]\n","gt_viz = []\n","pred_viz = []\n","for fi in test_fis:\n","  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n","  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n","  Y_pred = dino_segmentation.inference(X_test)\n","  iou = compute_iou(Y_pred, Y_test, num_classes)\n","  ious.append(iou)\n","\n","  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n","  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n","\n","gt_viz = np.concatenate(gt_viz, 1)\n","pred_viz = np.concatenate(pred_viz, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AH5KOhQY4JCb","test":"iou_accuracy"},"outputs":[],"source":["print(f\"Mean IoU on first test frames: {ious[0]:.3f}\")  # should be >0.45\n","print(f\"Mean IoU on last test frames: {ious[2]:.3f}\")  # should be >0.50"]},{"cell_type":"markdown","metadata":{"id":"_24lnmJ4FtvA"},"source":["Now let's visualize the results. Run the two cells below to display the ground truth and predicted segmentation masks for the first, middle, and last frames. Note that the middle frame is part of the training set, while the other frames are unseen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qai7ItCa7P-D"},"outputs":[],"source":["Image.fromarray(gt_viz)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxXEu--C7WYQ"},"outputs":[],"source":["Image.fromarray(pred_viz)"]},{"cell_type":"markdown","metadata":{"id":"gmD1T5UoGVHu"},"source":["Now run the following three cells to evaluate and visualize the entire video. You should achieve a mean IoU greater than 0.55. The saved visualization video may take some time to process in Google Drive, but you can download it to your computer and view it locally.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIRVNKs3oVPU"},"outputs":[],"source":["# Run on all frames\n","ious = []\n","gt_viz = []\n","pred_viz = []\n","for fi in range(len(frames)):\n","  if fi % 20 == 0:\n","    print(f\"{fi} / {len(frames)}\")\n","  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n","  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n","  Y_pred = dino_segmentation.inference(X_test)\n","  iou = compute_iou(Y_pred, Y_test, num_classes)\n","  ious.append(iou)\n","\n","  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n","  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n","\n","gt_viz = np.stack(gt_viz)  # T x H x W x 3\n","pred_viz = np.stack(pred_viz)  # T x H x W x 3\n","final_viz = np.concatenate([gt_viz, pred_viz], -2)  # T x H x 2W x 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCZkYXtEGkq2","test":"all_frames_iou"},"outputs":[],"source":["print(f\"Mean IoU on all frames: {sum(ious) / len(ious):.3f}\")  # should be >0.55\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3DCRaiI8V7J"},"outputs":[],"source":["def write_video_from_array(array, output_path, fps = 12):\n","    T, H, W, _ = array.shape\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n","    for i in range(T):\n","        frame = array[i]\n","        out.write(frame)\n","    out.release()\n","    print(f\"Video saved to {output_path}\")\n","\n","\n","# It might take a while to process in google drive but you can just download it and watch on your computer\n","write_video_from_array(final_viz, f\"/content/drive/My Drive/{FOLDERNAME}/dino_res.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"s5Z_N_lkHlPW"},"source":["**Inline Question 5** -\n","\n","If you train a segmentation model on CLIP ViT's patch features, do you expect it to perform better or worse than DINO? Why should that be the case?\n","\n","\n","\n","$\\color{blue}{\\textit Your Answer:}$\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7joEne5LNYB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1veMUkShr38Pj6B_7XgEdtioxI6sVfXkU","timestamp":1747174681237},{"file_id":"1EAsfgn8wpiEIIubKR4KuQKxsdCp2rtY2","timestamp":1747090108952}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}