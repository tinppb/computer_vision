{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mT0OBbGm6GKS"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = \"cs231n/assignments/assignment3/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# # This downloads the Emoji dataset to your Drive\n","# # if it doesn't already exist.\n","# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","# !bash get_datasets.sh\n","# %cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"markdown","metadata":{"id":"TobzYqpo8xyK"},"source":["# Denoising Diffusion Probabilistic Models\n","\n","So far, we have explored discriminative models, which are trained to produce a labeled output. These range from straightforward image classification to sentence generation where the problem was still framed as classification, with labels in vocabulary space and a recurrence mechanism capturing multi-word labels. Now, we will expand our repertoire by building a generative model capable of creating novel images that resemble a given set of training images.\n","\n","There are many types of generative models, including Generative Adversarial Networks (GANs), autoregressive models, normalizing flow models, and Variational Autoencoders (VAEs), all of which can synthesize striking images. However, in 2020, Ho et al. introduced Denoising Diffusion Probabilistic Models (DDPMs) by combining diffusion probabilistic models with denoising score matching. This resulted in a generative model that is both simple to train and powerful enough to generate complex, high-quality images. The following provides a high-level overview of DDPMs. For more details, please refer to the course slides and the original DDPM paper [1].\n","\n","\n","# Forward Process\n","Let $q(x_0)$ be the distribution of clean dataset images. We define a forward noising process as a Markov chain of small noising steps:\n","\n","$$q(x_t | x_{t-1}) \\sim N(x_t; \\sqrt{1-\\beta_t} x_{t-1} , \\beta_t I)$$\n","\n","where the stepwise variance $(\\beta_1, ..., \\beta_T)$ determines the noise schedule. Due to the properties of Gaussian distributions, we can express $q(x_t | x_0)$ in closed form as:\n","\n","$$q(x_t | x_0) \\sim N(x_t; \\sqrt{\\bar{\\alpha}_t} x_0 , (1-\\bar{\\alpha}_t) I)$$\n","\n","where $\\alpha_t = 1-\\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t}\\alpha_t$.\n","If the noise schedule $(\\beta_1, ..., \\beta_T)$ is set properly, the final distribution $q(x_T)$ becomes indistinguishable from pure Gaussian noise $N(0, I)$.\n","\n","Recall that sampling from a Gaussian distribution $x \\sim N(\\mu, \\sigma^2)$ is equivalent to computing $\\sigma * \\epsilon + \\mu$ where $\\epsilon \\sim N(0, 1)$. Hence, sampling from $q(x_t | x_{t-1})$ or $q(x_t | x_0)$ is straightforward given $x_{t-1}$ or $x_0$ respectively. Because of this, the forward process is simple and does not require learning.\n","\n","# Reverse Process\n","The reverse process reconstructs a clean image $x_0$ from pure noise $x_T$ through multiple steps. Let $p(x_{t-1} | x_t)$ denote the reverse step of $q(x_t | x_{t-1})$.\n","The first key insight is that learning to reverse each individual denoising step is easier than reversing the entire forward process in one go. In other words, learning $p(x_{t-1} | x_t)$ for each $t$ is easier than directly learning $p(x_0 | x_T)$.\n","\n","However, learning $p(x_{t-1} | x_t)$ is still challenging. Although $q(x_t | x_{t-1})$ is Gaussian, $p(x_{t-1} | x_t)$ could take any complex form and is almost certainly not Gaussian. Modeling and sampling from an arbitrary distribution is significantly harder than working with a simple parametric distribution like a Gaussian.\n","\n","The second key insight is that if the stepwise noise $\\beta_t$ in the forward process is small enough, then the reverse step $p(x_{t-1} | x_t)$ is also close to a Gaussian distribution. Thus, we only need to estimate its mean and variance. In practice, setting the variance of $p(x_{t-1} | x_t)$ to match $\\beta_t$ (the same as in the forward step) works well. Consequently, learning the reverse process reduces to learning the mean $\\mu(x_t, t; \\theta)$ where $\\theta$ represents the parameters of a neural network.\n","\n","# Denoising Objective\n","Generative models are optimized by minimizing the expected negative log-likelihood $\\mathbb{E}[-\\log{p_\\theta(x_0)}]$ of the dataset samples. The likelihood of each sample can be expressed as: $p_\\theta(x_0) = p(x_T)\\prod_{t=1}^T p(x_{t-1} | x_t)$. Since this objective is intractable in many cases, various classes of generative models optimize the variational lower bound on the negative log-likelihood.\n","\n","Ho et al. demonstrated that this objective is equivalent to minimizing the following denoising loss\n","\n","$$\\mathbb{E}_{t, x_0, \\epsilon}\\left[ \\| \\epsilon - \\epsilon_\\theta (\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t ) \\|^2 \\right]$$\n","\n","where $t$ is uniform between 1 and T, $x_0$ is clean sample, $\\epsilon$ is sampled from standard gaussian $N(0, I)$, and $\\epsilon_\\theta$ is a neural network model trained to predict the noise $\\epsilon$ from the input noisy sample $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$. In other words, $\\epsilon_\\theta$ learns to denoise the input noisy image. Note that this is equivalent to predicting the clean sample, since the noise can be recovered from the noisy image and the clean sample following the equation $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$.\n","\n","\n","[1] Denoising Diffusion Probabilistic Models. Jonathan Ho, Ajay Jain, Pieter Abbeel. [Link](https://arxiv.org/pdf/2006.11239)\n","\n"]},{"cell_type":"markdown","source":["# In This Notebook...\n","We will implement and train a DDPM model to generate small 32 x 32 emoji images conditioned on text prompts. First, we will implement the forward noising process based on Eq. (4) of the paper [1]. Then we will build a UNet model that takes $x_t$ and $t$ as inputs (optionally with other conditioning like text-prompt) and outputs a tensor of the same shape as $x_t$. Finally, we will implement the denoising objective and train our DDPM model.\n","\n","We use the text encoder from a pretrained CLIP[2] model to encode input text into a 512-dimensional vector. To speed up training, we've already pre-encoded the text data from the training set.\n","\n","[2] Learning transferable visual models from natural language supervision. Radford et. al. [Link](https://github.com/openai/CLIP)"],"metadata":{"id":"mcKJdqTt3wqV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXLbugEj4jnn"},"outputs":[],"source":["!pip install git+https://github.com/openai/CLIP.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTjwZxfhX6Yr"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import numpy as np\n","import torch\n","import random\n","import matplotlib.pyplot as plt\n","import torchvision.utils as tv_utils\n","from cs231n.emoji_dataset import EmojiDataset\n","from cs231n.gaussian_diffusion import GaussianDiffusion\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error.\"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-10, np.abs(x) + np.abs(y))))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kb5fIDb2667T"},"outputs":[],"source":["# First, let's load and visualize the dataset.\n","# Each sample of the dataset is a tuple: (image, {\"text_emb\": <tensor>, \"text\": <string>})\n","# We will use a pretrained text-encoder to encode text into an embedding vector.\n","# We have pre-encoded the dataset texts into embeddings for faster training.\n","image_size = 32\n","dataset = EmojiDataset(image_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icgscLv6WHy2"},"outputs":[],"source":["def visualize_samples(dataset, num_samples=25, grid_size=(5, 5)):\n","    # Randomly sample indices\n","    indices = random.sample(range(len(dataset)), num_samples)\n","    samples = [dataset[i] for i in indices]\n","\n","    # Inspect one sample\n","    img_shape = list(samples[0][0].shape)\n","    emb_shape = list(samples[0][1][\"text_emb\"].shape)\n","    print(f\"One sample: (image: {img_shape}, {{ \\\"text_emb\\\": {emb_shape}, \\\"text\\\": string }})\")\n","\n","    # Extract images and texts\n","    images = torch.stack([sample[0] for sample in samples])  # Stack images\n","    texts = [sample[1][\"text\"] for sample in samples]  # Extract text descriptions\n","\n","    # Create a grid of images\n","    grid_img = tv_utils.make_grid(images, nrow=grid_size[1], padding=2)\n","\n","    # Convert to numpy for plotting\n","    grid_img = grid_img.permute(1, 2, 0).numpy()\n","\n","    # Plot the images\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    ax.imshow(grid_img)\n","    ax.axis(\"off\")\n","\n","    # Add text annotations\n","    grid_w, grid_h = grid_size\n","    img_w, img_h = grid_img.shape[1] // grid_w, grid_img.shape[0] // grid_h\n","\n","    for i, text in enumerate(texts):\n","        row, col = divmod(i, grid_w)\n","        x, y = col * img_w, row * img_h\n","        ax.text(x+5, y+5, text[:30], fontsize=8, color='white', bbox=dict(facecolor='black', alpha=0.5))\n","\n","    plt.show()\n","\n","visualize_samples(dataset)"]},{"cell_type":"markdown","metadata":{"id":"574WWI4adHJ8"},"source":["## q_sample\n","\n","Now we will define the forward noising process. Read through the GaussianDiffusion class in `cs231n/gaussian_diffusion.py`. Consult the original DDPM paper[1] for the equations. Implement `q_sample` method and test it below. You should see zero relative error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"An3PqQNDgzby"},"outputs":[],"source":["# Test GaussianDiffusion.q_sample method\n","sz = 2\n","b = 3\n","\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=sz,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n",")\n","\n","t = torch.tensor([0, 300, 999]).long()\n","x_start = torch.linspace(-0.9, 0.6, b*3*sz*sz).view(b, 3, sz, sz)\n","noise = torch.linspace(-0.7, 0.8, b*3*sz*sz).view(b, 3, sz, sz)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","\n","expected_x_t = np.array([\n","    [\n","        [[-0.9119949, -0.86840147], [-0.8248081, -0.7812148]],\n","        [[-0.7376214, -0.694028], [-0.65043473, -0.6068413]],\n","        [[-0.563248, -0.51965463], [-0.47606122, -0.43246788]],\n","    ],\n","    [\n","        [[-0.42800453, -0.37039882], [-0.31279305, -0.2551873]],\n","        [[-0.19758154, -0.1399758], [-0.08237009, -0.024764337]],\n","        [[0.032841414, 0.090447165], [0.14805292, 0.20565866]],\n","    ],\n","    [\n","        [[0.32864183, 0.37152246], [0.41440308, 0.45728368]],\n","        [[0.50016433, 0.5430449], [0.5859255, 0.6288062]],\n","        [[0.67168677, 0.7145674], [0.757448, 0.8003287]],\n","    ],\n","]).astype(np.float32)\n","\n","# Should see zero relative error\n","print(\"x_t error: \", rel_error(x_t.numpy(), expected_x_t))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DBoYTeqYNan"},"outputs":[],"source":["# Let's visualize the noisy images at various timesteps.\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=image_size,\n","      timesteps=1000,\n",")\n","\n","B = 10\n","img = dataset[770][0]  # 3 x H x W\n","x_start = img[None].repeat(B, 1, 1, 1)  # B x 3 x H x W\n","noise = torch.randn_like(x_start)  # B x 3 x H x W\n","t = torch.linspace(0, 1000-1, B).long()\n","\n","x_start = diffusion.normalize(x_start)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","x_t = diffusion.unnormalize(x_t).clamp(0, 1)\n","grid_img = tv_utils.make_grid(x_t, nrow=5, padding=2)\n","grid_img = grid_img.permute(1, 2, 0).cpu().numpy()\n","fig, ax = plt.subplots(figsize=(10, 10))\n","ax.imshow(grid_img)\n","ax.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TjFcb4iUlo81"},"source":["A diffusion model can be trained to predict either the clean image or the noise, as one can be derived from the other (explained in 'Denoising Objective' section above). Implement `predict_start_from_noise` and `predict_noise_from_start` methods and test them below. You should see relative error less than 1e-5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ1329nKfxs4"},"outputs":[],"source":["# Test `predict_noise_from_start` and `predict_start_from_noise`\n","sz = 2\n","b = 3\n","\n","diffusion = GaussianDiffusion(\n","      model=None,\n","      image_size=sz,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n",")\n","\n","t = torch.tensor([1, 300, 998]).long()\n","x_start = torch.linspace(-0.91, 0.67, b*3*sz*sz).view(b, 3, sz, sz)\n","noise = torch.linspace(-0.73, 0.81, b*3*sz*sz).view(b, 3, sz, sz)\n","x_t = diffusion.q_sample(x_start, t, noise)\n","\n","pred_noise = diffusion.predict_noise_from_start(x_t, t, x_start)\n","pred_x_start = diffusion.predict_start_from_noise(x_t, t, noise)\n","\n","# Should relative errors around 1e-5 or less\n","print(\"noise error: \", rel_error(pred_noise.numpy(), noise.numpy()))\n","print(\"x_start error: \", rel_error(pred_x_start.numpy(), x_start.numpy()))"]},{"cell_type":"markdown","metadata":{"id":"f6vzbk91oNFK"},"source":["## UNet Model\n","\n","Now that we have defined the forward process, let's define the UNet model for denoising the input image. UNet is a neural network architecture designed for image-to-image tasks like segmentation, style transfer, etc. It consists of an encoder (or downsampling module) that transforms the input image into hierarchical features with decreasing spatial resolution and increasing feature dimensions. The decoder (or upsampling module) then upsamples the features by progressively restoring the spatial resolution, mirroring the encoder's structure. At each decoder layer, features from the corresponding encoder layer are concatenated, providing a direct pathway for finer details. This approach reduces the burden on the bottleneck layers, allowing them to focus on capturing high-level representations rather than memorizing fine details.\n","\n","We will use UNet in this case because both our input and output are aligned images with same dimensions: C x H x W. Each ResNet block in the UNet will also take an additional input vector called context for conditioning. We will generate the context vector by encoding the diffusion timestep and text-prompt.\n","\n","Run the cell below to get a rough outline of the UNet architecture. Each red box represents a ResNet block containing 2 or 3 convolutional layers that maintain the spatial resolution of the feature maps. The context vector input to every ResNet block is ommitted for clarity. The shape written below each box indicates the output tensor shape after that block. Additional arrows illustrate the skip connections, which enable the U-Net to preserve fine-grained details in the output. For example, the output of `layer1_block1` with shape (d, h, w) will be concatenated with the output of `layer4_block1`, also with shape (d, h, w), before being passed to `layer4_block2`. Therefore, `layer4_block2` will receive an input of shape (2*d, h, w)."]},{"cell_type":"code","source":["from IPython.display import Image\n","Image(f'/content/drive/My Drive/{FOLDERNAME}/unet.png')"],"metadata":{"id":"sCiRSdNTGaej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8UEQ67aQj-v"},"source":["Implement the `Unet.__init__` method in `cs231n/unet.py` to define the upsampling and downsampling blocks of the UNet model, and then test it below. If your implementation is correct, you should not see any error. Calling `Unet(dim=d, condition_dim=condition_dim, dim_mults=(2,4))` should successfully create a UNet model corresponding to the architecture shown in the figure above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2AWEkjLnv58"},"outputs":[],"source":["from cs231n.unet import Unet, ResnetBlock, Downsample, Upsample\n","\n","dim = 2\n","condition_dim = 4\n","dim_mults = (1, 2, 4)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","\n","# Check number of layers\n","assert len(unet.downs) == len(dim_mults), \"Number of Unet downsampling blocks is wrong.\"\n","assert len(unet.ups) == len(dim_mults), \"Number of Unet upsampling blocks is wrong.\"\n","\n","\n","# Check layers\n","try:\n","  expected_downs_dims = [2, 2, 8, 2, 2, 8, 2, 2, 8, 2, 2, 8, 4, 4, 8, 4, 4, 8]\n","  downs_dims = [\n","      d for m in unet.downs for d in [\n","          m[0].dim, m[0].dim_out, m[0].context_dim, m[1].dim, m[1].dim_out, m[1].context_dim,\n","      ]\n","  ]\n","  assert downs_dims == expected_downs_dims, \"Dimensions don't match\"\n","except Exception as e:\n","  raise RuntimeError(\"Downsampling blocks wrongly configured\") from e\n","\n","\n","try:\n","  expected_ups_dims = [8, 4, 8, 8, 4, 8, 4, 2, 8, 4, 2, 8, 4, 2, 8, 4, 2, 8]\n","  ups_dims = [\n","      d for m in unet.ups for d in [\n","          m[1].dim, m[1].dim_out, m[1].context_dim, m[2].dim, m[2].dim_out, m[2].context_dim,\n","      ]\n","  ]\n","  assert ups_dims == expected_ups_dims, \"Dimensions don't match\"\n","except Exception as e:\n","  raise RuntimeError(\"Upsampling blocks wrongly configured\") from e\n","\n","# Check number of parameters\n","num_params = sum(p.numel() for p in unet.parameters())\n","expected_num_params = 6499\n","assert num_params == expected_num_params, \"Unet model creation is wrong!\"\n"]},{"cell_type":"markdown","metadata":{"id":"Ml9084MZYB4l"},"source":["Fill in `Unet.forward` method in `cs231n/unet.py` and test it below. For now, don't worry about `Unet.cfg_forward` method. You should see relative error less than 1e-5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vw20YCKLNO2-"},"outputs":[],"source":["np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2, 4)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","b = 2\n","h = w = 4\n","inp_x = torch.randn(b, 3, h, w)\n","inp_text_emb = torch.randn(b, condition_dim)\n","inp_t = torch.tensor([8, 25]).long()\n","out = unet.forward(x=inp_x, time=inp_t,\n","                   model_kwargs={\"text_emb\": inp_text_emb}).detach().numpy()\n","\n","expected_out = np.array(\n","      [[[[ 0.14615417,  0.36610782,  0.27948245,  0.2229169 ],\n","         [ 1.0268314 , -0.04441035,  0.33097324,  0.21493062],\n","         [ 0.15944722,  1.1060286 ,  0.36489075,  0.39395577],\n","         [ 0.5593624 ,  0.95084137,  0.46409354, -0.15076232]],\n","\n","        [[ 0.07152754, -0.19067341,  0.36995906,  0.1898715 ],\n","         [ 0.18764025, -0.37758452,  0.22994985,  0.14644745],\n","         [ 0.39364466,  0.42091975,  0.75438905, -0.17806   ],\n","         [ 0.0934296 ,  0.44165182,  0.2768886 ,  0.19760622]],\n","\n","        [[ 0.39873862,  0.86417544,  0.707601  ,  0.5136454 ],\n","         [ 0.8151177 ,  0.01816908,  0.64427924,  0.45256743],\n","         [ 0.6901425 ,  1.0449984 ,  0.8272561 ,  0.38516602],\n","         [ 0.48775655,  0.91759497,  0.56286275,  0.38452417]]],\n","\n","\n","       [[[ 0.31076878,  0.25998223,  0.35973004, -0.01464513],\n","         [ 0.37456402,  0.10733554,  1.1211727 ,  0.596719  ],\n","         [-0.19628221,  0.49115434,  0.5591996 , -0.02811927],\n","         [ 0.2980889 ,  0.7983323 ,  0.31545636,  0.1045265 ]],\n","\n","        [[-0.21484727, -0.11434001,  0.01019827, -0.07907221],\n","         [-0.14186645,  0.2666731 ,  0.36379665,  0.25780094],\n","         [ 0.6618308 ,  0.09432775,  0.3441353 ,  0.11780772],\n","         [ 0.3818162 ,  0.54577625,  0.15127666,  0.2136025 ]],\n","\n","        [[ 0.23299581,  0.51728034,  0.5330554 ,  0.30019608],\n","         [ 0.34902877,  0.29055628,  1.1447697 ,  0.5087651 ],\n","         [ 0.7447357 ,  0.4974355 ,  0.564866  ,  0.4631402 ],\n","         [ 0.6024195 ,  0.8882342 ,  0.46354175,  0.4344969 ]]]])\n","\n","print(\"forward error: \", rel_error(out, expected_out))"]},{"cell_type":"markdown","metadata":{"id":"xIr9B78lQFdg"},"source":["# p_losses\n","\n","Now that we have model implementation done, let's write the DDPM's denoising training step. As mentioned before, optimizing the denoising loss is equivalent to minimizing the expected negative log likelihood of the dataset. Complete the `GaussianDiffusion.p_losses` method in `cs231n/gaussian_diffusion.py` and test it below. You should see relative error less than 1e-6 ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6QyU74ISQKR"},"outputs":[],"source":["np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2, 4)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","h = w = 4\n","b = 3\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n","      objective=\"pred_x_start\",\n",")\n","\n","inp_x = torch.randn(b, 3, h, w)\n","inp_model_kwargs = {\"text_emb\": torch.randn(b, condition_dim)}\n","out = diffusion.p_losses(inp_x, inp_model_kwargs)\n","expected_out = 30.0732689\n","\n","print(\"forward error: \", rel_error(out.item(), expected_out))"]},{"cell_type":"markdown","source":["## p_sample\n","\n","There is one final ingredient remaining now. DDPM generates samples by iteratively performing the reverse process. Each iteration of this reverse process involves sampling from $p(x_{t-1}|x_t)$. Open `cs231n/gaussian_diffusion.py` and implement `p_sample` method by following Equation (6) from the paper. This equation describes sampling from the posterior of the forward process, conditioned on $x_t$ and $x_0$, where $x_0$ can be derived from the denoising model's output. We have already implemented `sample` method that iteratively calls `p_sample` to generate images from input texts.\n","\n","Test your implementation of `p_sample` below, you should see relative errors less than 1e-6."],"metadata":{"id":"h5o1x1MjS20i"}},{"cell_type":"code","source":["np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2,)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","h = w = 4\n","b = 1\n","inp_x_t = torch.randn(b, 3, h, w)\n","inp_model_kwargs = {\"text_emb\": torch.randn(b, condition_dim)}\n","t = 231\n","\n","# test 1\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"sigmoid\",\n","      objective=\"pred_x_start\",\n",")\n","out = diffusion.p_sample(inp_x_t, t, inp_model_kwargs).detach().numpy()\n","expected_out = np.array(\n","    [[[[ 1.1339471 ,  0.12097352, -0.7175048 ,  1.3196243 ],\n","         [-0.27657282,  0.4899886 ,  1.0170169 , -0.8242867 ],\n","         [-0.18946372,  0.9899801 ,  0.01498353,  0.39722288],\n","         [-0.97995025, -0.5947938 , -0.07796463, -0.07311387]],\n","\n","        [[ 0.0739838 , -1.5537696 ,  0.43128064, -0.7395982 ],\n","         [-1.0517508 , -1.7030833 ,  0.79073197, -1.217138  ],\n","         [-0.5314434 ,  0.9862699 ,  0.6568664 , -0.4559122 ],\n","         [-0.17322278,  0.51251256, -0.75741345, -0.3967054 ]],\n","\n","        [[ 0.8546979 ,  1.6186953 ,  1.9930652 ,  0.57347   ],\n","         [ 0.20219846,  0.5374655 , -0.81597316,  1.9089762 ],\n","         [ 0.7327057 ,  1.19275   ,  1.8593936 , -1.4582647 ],\n","         [ 0.68447256, -0.9056745 ,  0.7863245 ,  0.14455058]]]])\n","print(\"forward error: \", rel_error(out, expected_out))\n","\n","# test 2\n","diffusion = GaussianDiffusion(\n","      model=unet,\n","      image_size=h,\n","      timesteps=1000,\n","      beta_schedule=\"cosine\",\n","      objective=\"pred_noise\",\n",")\n","out = diffusion.p_sample(inp_x_t, t, inp_model_kwargs).detach().numpy()\n","expected_out = np.array(\n","    [[[[ 1.1036711 ,  0.08143333, -0.6856102 ,  1.3826138 ],\n","         [-0.25455472,  0.514572  ,  1.104592  , -0.75972646],\n","         [-0.22729763,  0.9837706 ,  0.05891411,  0.52049375],\n","         [-1.0331786 , -0.5416254 , -0.01623197, -0.04838388]],\n","\n","        [[ 0.08324978, -1.545468  ,  0.41357145, -0.63511896],\n","         [-1.1362139 , -1.7128816 ,  0.8694859 , -1.2297069 ],\n","         [-0.49168122,  1.0043695 ,  0.6759953 , -0.5297671 ],\n","         [-0.10931232,  0.52347076, -0.80946106, -0.5015002 ]],\n","\n","        [[ 0.7437265 ,  1.590004  ,  1.9481117 ,  0.5656144 ],\n","         [ 0.22895451,  0.5289113 , -0.8511001 ,  1.8864397 ],\n","         [ 0.72863096,  1.2271638 ,  1.892699  , -1.5199479 ],\n","         [ 0.64346373, -0.86913294,  0.7869012 ,  0.12637165]]]])\n","print(\"forward error: \", rel_error(out, expected_out))"],"metadata":{"id":"5XhR3kYUS2BL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLsK_lmbSv2o"},"source":["## Training\n","\n","We have all ingredients needed for DDPM training and we can train the model on our Emoji dataset. You don't have to code anything here but we encourage you to look at the training code at `cs231n/ddpm_trainer.py`.\n","\n","For the rest of the notebook, we will use a pretrained model from `cs231n/exp/pretrained` folder which is already trained for many iterations on this dataset. However, you are free to train your own model on colab GPU (make sure to change the `results_folder`). Note that it may take more than 12 hours on T4 GPU before you start seeing a reasonable generation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkCY1rGnSzUF"},"outputs":[],"source":["from cs231n.ddpm_trainer import Trainer\n","\n","dim = 48\n","image_size = 32\n","results_folder = f\"/content/drive/My Drive/{FOLDERNAME}/cs231n/exp/pretrained\"\n","condition_dim = 512\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = Unet(\n","    dim=dim,\n","    dim_mults=(1, 2, 4, 8),\n","    condition_dim=condition_dim,\n",")\n","print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n","\n","diffusion = GaussianDiffusion(\n","    model,\n","    image_size=image_size,\n","    timesteps=100,  # number of diffusion steps\n","    objective=\"pred_noise\",  # \"pred_x_start\" or \"pred_noise\"\n",")\n","\n","dataset = EmojiDataset(image_size)\n","\n","trainer = Trainer(\n","    diffusion,\n","    dataset,\n","    device,\n","    train_batch_size=256,\n","    weight_decay=0.0,\n","    train_lr=1e-3,\n","    train_num_steps=50000,\n","    results_folder=results_folder,\n",")\n","\n","# You are not required to train it yourself.\n","# trainer.train()\n"]},{"cell_type":"code","source":["# Instead, we will load a pretrained model.\n","trainer.load(70000)"],"metadata":{"id":"1mgtWkkOSvze"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzZhxokqcfsC"},"outputs":[],"source":["# Helper function to get CLIP text embedding during inference time.\n","from cs231n.emoji_dataset import ClipEmbed\n","clip_embedder = ClipEmbed(device)\n","def get_text_emb(text):\n","    return trainer.ds.embed_new_text(text, clip_embedder)\n","\n","# Helper function to visualize generations.\n","def show_images(img):\n","    # img: B x T x 3 x H x W\n","    plt.figure(figsize=(10, 10))\n","    img2 = img.clamp(0, 1).permute(0, 3, 1, 4, 2).flatten(0, 1).flatten(1, 2).cpu().numpy()\n","    plt.imshow(img2)\n","    plt.axis('off')\n","\n","    plt.show()"]},{"cell_type":"markdown","source":["## Sampling\n","\n","Run the cell below to visualize emoji generations conditioned on a text prompt. Feel free to modify the prompt to explore different generations. Since our emoji dataset is quite small, it is insufficient to train a fully generalizable text-to-image model. Because of that, generations for unseen prompts may be poor or may not be faithful to the input text (this may also happen for seen examples but less common).\n","\n","For faster sampling, you may use a GPU runtime. If you switch the runtime type, be sure to rerun the entire notebook."],"metadata":{"id":"PeGUjQuinelo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkwPx3vQz0LQ"},"outputs":[],"source":["# text = \"crying face\"  # seen example, good generations\n","text = \"face with cowboy hat\"  # seen example, good generations\n","# text = \"crying face with cowboy hat\"  # unseen example, bad generations\n","text_emb = get_text_emb(text)\n","text_emb = text_emb[None].expand(5, -1).to(device)\n","\n","\n","img = trainer.diffusion_model.sample(\n","    batch_size=5,\n","    model_kwargs={\"text_emb\": text_emb},\n","    return_all_timesteps=True\n",")\n","show_images(img[:, ::20])"]},{"cell_type":"markdown","source":["## Classifier Free Guidance\n","\n","Generative models are typically evaluated on fidelity (i.e., the quality or realism of the generated samples) and diversity (the variability or coverage of the sample space). For conditional generative models, fidelity additionally refers to how faithfully the generated samples adhere to the input conditions. These two metrics are often in tension with each other, leading to a trade-off between them. Ho et al. introduced a simple technique called classifier-free guidance [3], which allows explicit control over this trade-off.\n","\n","\n","In classifier-free guidance, during the training of a conditional diffusion model $\\epsilon_\\theta(x_t, t, c)$ , the condition $c$ is randomly dropped (i.e. replaced with $c=\\phi$) with some probability (typically 0.1 to 0.2). During each denoising step of sampling, the prediction is updated as:\n","$$\\epsilon_\\theta(x_t, t, c) \\leftarrow (w+1) \\epsilon_\\theta(x_t, t, c) - w \\epsilon_\\theta(x_t, t, \\phi)$$\n","where $w$ is a positive scalar (the guidance scale). In other words, we perform two predictions during each denoising step, one conditional and one unconditional, and combine them linearly to favor the conditional generation. $w$ is a hyperparameter that is tuned to optimize a model-specific evaluation metric. A higher $w$ makes the generations more faithful to the condition but tends to reduce their diversity.\n","\n","[3] Classifier-Free Diffusion Guidance. Jonathan Ho, Tim Salimans. [Link](https://arxiv.org/abs/2207.12598)"],"metadata":{"id":"p_JGyu2UyRRx"}},{"cell_type":"markdown","source":["Implement the classifier-free guidance in `Unet.cfg_forward` method in `cs231n/unet.py` and test it below. You should see relative error less than 1e-6."],"metadata":{"id":"HJzlBhTy3v1t"}},{"cell_type":"code","source":["np.random.seed(231)\n","torch.manual_seed(231)\n","\n","dim = 4\n","condition_dim = 4\n","dim_mults = (2, 4)\n","unet = Unet(dim=dim, condition_dim=condition_dim, dim_mults=dim_mults)\n","\n","b = 2\n","h = w = 4\n","inp_x = torch.randn(b, 3, h, w)\n","inp_text_emb = torch.randn(b, condition_dim)\n","inp_t = torch.tensor([8, 25]).long()\n","out = unet.forward(x=inp_x, time=inp_t,\n","                   model_kwargs={\"text_emb\": inp_text_emb, \"cfg_scale\": 2.31}\n","                   ).detach().numpy()\n","\n","expected_out = np.array(\n","      [[[[-0.07755187,  0.39913225, -0.616872  ,  0.16161466],\n","         [ 0.76309466, -0.64505696,  1.1228579 ,  0.1429432 ],\n","         [-0.58470994,  1.5556629 ,  0.19990933,  0.6726817 ],\n","         [ 0.34811258,  1.6286248 , -0.57835865, -0.3712303 ]],\n","\n","        [[-0.2780811 ,  0.09640026,  0.80653083,  0.3257922 ],\n","         [ 0.49113247, -1.2000966 ,  0.9383536 ,  0.10577369],\n","         [ 0.5326107 ,  0.38000846,  0.90770614,  0.08911347],\n","         [-0.2537056 ,  0.6668851 , -0.16009146,  0.4560123 ]],\n","\n","        [[-0.03857625,  1.2413033 ,  0.89891887,  0.22149336],\n","         [ 0.9030682 , -1.0636187 ,  1.2424004 ,  0.56415176],\n","         [ 0.6789831 ,  1.367657  ,  0.84504557,  0.5781751 ],\n","         [ 0.10814822,  1.3854939 , -0.33456588,  0.34210002]]],\n","\n","\n","       [[[ 0.17439526, -0.01185328,  0.39814878,  0.2655859 ],\n","         [ 0.1156677 , -0.29466197,  4.5019875 ,  0.90760195],\n","         [-0.7210121 ,  0.32611835,  1.262263  , -0.46243155],\n","         [ 0.05207008,  1.3481442 ,  0.06369245,  0.46200275]],\n","\n","        [[-0.24512854, -0.08326203,  0.04366357, -0.86336297],\n","         [-0.9094473 ,  0.36758858,  0.5417196 ,  0.33162278],\n","         [ 1.233382  ,  0.4753497 ,  1.0248462 , -0.1512323 ],\n","         [ 0.40446353,  0.77949953, -0.48068368,  0.92509973]],\n","\n","        [[ 0.22089547,  0.43676746,  0.31286478,  0.273731  ],\n","         [-0.34253466, -0.18519384,  2.603891  ,  0.6012087 ],\n","         [ 1.2847279 ,  0.8032987 ,  1.0297089 ,  0.52087414],\n","         [ 0.5678704 ,  1.1869694 ,  0.09395003,  0.90305966]]]])\n","\n","\n","print(\"forward error: \", rel_error(out, expected_out))"],"metadata":{"id":"ruOLxF5dVIq8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the cell below to visualize emoji generations using classifier-free guidance. Feel free to modify the \"cfg_scale\" parameter value as well. As mentioned earlier, since our model does not generalize well, you may or may not observe faithful generations even with a high guidance scale.\n","\n"],"metadata":{"id":"OYUOMGBb4hOE"}},{"cell_type":"code","source":["# text = \"crying face\"  # seen example, good generations\n","text = \"face with cowboy hat\"  # seen example, good generations\n","# text = \"crying face with cowboy hat\"  # unseen example, bad generations\n","text_emb = get_text_emb(text)\n","text_emb = text_emb[None].expand(5, -1).to(device)\n","\n","\n","img = trainer.diffusion_model.sample(\n","    batch_size=5,\n","    model_kwargs={\"text_emb\": text_emb, \"cfg_scale\": 1},\n","    return_all_timesteps=True\n",")\n","show_images(img[:, ::20])"],"metadata":{"id":"qE13WHe94M3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cntA7to052FS"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1cnMLPY-3W9pkW1dLX3gsfnNatq3JE8x1","authorship_tag":"ABX9TyOOruySVzxLVrpjk/JTsh5S"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}